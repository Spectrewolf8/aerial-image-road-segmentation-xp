{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1516242,"sourceType":"datasetVersion","datasetId":893591}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install datasets\n!pip install wurlitzer==3.1.1\n!pip install tensorflow==2.14.1\n!pip install keras==2.14.0\n!pip install ml-dtypes==0.2.0\n!pip install pydantic==1.10.11\n\n!pip install huggingface_hub","metadata":{"_uuid":"e8977b83-ec89-4624-aa7c-1e7b9672b22f","_cell_guid":"7e7a4ec0-365a-4320-aaaa-d7586105c62f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Important: You must restart the kernel at this point after installing the packages!!\n`This notebook is written and targetted towards kaggle`\n\n---","metadata":{}},{"cell_type":"markdown","source":"# Developing Model and preparing Dataset\n### Model architecture: U-NET\n### Dataset used: massachusetts-road-segmentation","metadata":{}},{"cell_type":"code","source":"# impoting classes\n\nimport os\nimport cv2\nimport h5py\nimport keras\nimport shutil\nimport tifffile\nimport tensorflow\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\nfrom tensorflow.keras.models import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Dropout","metadata":{"_uuid":"bd3d4439-b466-4745-9827-c82f828cdb14","_cell_guid":"0748d7f6-08ab-46b2-8b49-7447066f2eba","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing Directories","metadata":{}},{"cell_type":"code","source":"#Creating necessary directories if they don't exist already based on flag\n\n# Flag to determine if directories should be recreated\nrecreate_dirs = True\n\n# Directories for training data\ntrain_image_dir = \"/kaggle/working/train/images\"\ntrain_label_dir = \"/kaggle/working/train/labels\"\n\n# Directories for testing data\ntest_image_dir = \"/kaggle/working/test/images\"\ntest_label_dir = \"/kaggle/working/test/labels\"\n\n# Directories for validation data\nval_image_dir = \"/kaggle/working/val/images\"\nval_label_dir = \"/kaggle/working/val/labels\"\n\n# List of directories to create or recreate\ndirectories = ['/kaggle/working/train',\n               '/kaggle/working/test',\n               '/kaggle/working/val',\n    train_image_dir, train_label_dir,\n    test_image_dir, test_label_dir,\n    val_image_dir, val_label_dir\n]\n\n# Create or recreate directories\nfor dir_path in directories:\n    if recreate_dirs:\n        if os.path.exists(dir_path):\n            shutil.rmtree(dir_path)  # Remove the directory and its contents\n        os.makedirs(dir_path)  # Recreate the directory\n    else:\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)  # Create the directory if it doesn't exist\n","metadata":{"_uuid":"cfb3f48e-6b31-4a6a-a841-7206af78d8eb","_cell_guid":"d815fdd0-ca3b-4e98-8736-b1635d2a1a8e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining image directory variables and crop size\noutput_train_image_dir = \"/kaggle/working/train/images\"\noutput_train_label_dir = \"/kaggle/working/train/labels\"\n\noutput_test_image_dir = \"/kaggle/working/test/images\"\noutput_test_label_dir = \"/kaggle/working/test/labels\"\n\noutput_val_image_dir = \"/kaggle/working/val/images\"\noutput_val_label_dir = \"/kaggle/working/val/labels\"\n\nCROP_SIZE = 256","metadata":{"_uuid":"72a415d5-fad5-46dd-b349-74d38d46f9ac","_cell_guid":"0f170b27-bd86-4d98-be18-eaa6b109cf63","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing Dataset","metadata":{}},{"cell_type":"code","source":"# Defining necessary methods for preparing datasets\ndef list_files(folder, extension='.tiff'):\n    filenames = os.listdir(folder)\n    filepaths = [os.path.join(folder, filename) for filename in filenames]\n    return filepaths\n\n#load images as numpy arrays\ndef load_image(file_path):\n    tiff_data = tifffile.imread(file_path)\n    np_array = np.array(tiff_data)\n    return np_array\n\n# Converts grayscale masks to single channel binary b/w masks\ndef binarize(image):\n    _, binary = cv2.threshold(image, 0, 1, cv2.THRESH_BINARY)\n    return binary\n\n# Crop original 1500x1500 images to 256x256 crops\ndef create_crops(image, label, size=256):\n    crops = []\n    labels = []\n    h, w, _ = image.shape\n    for i in range(0, h - size + 1, size):\n        for j in range(0, w - size + 1, size):\n            crop_img = image[i:i+size, j:j+size]\n            crop_label = label[i:i+size, j:j+size]\n            if np.mean(crop_label) > 0.01 and 0 < np.mean(crop_img) < 255:  # Checking if label is not 99% black\n                crops.append(crop_img)\n                labels.append(crop_label)\n    return crops, labels","metadata":{"_uuid":"b7513a6e-1b03-442e-b6d2-c483ae76d663","_cell_guid":"b16c3853-42ed-4f2a-bd32-a43dd56705a7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the method for cropping original dataseet images into 256x256 crops and saving them in the respective directories\ndef crop_and_map_labels(output_image_dir,output_label_dir, input_dir):\n    \n        count = 0\n        image_folder = f'{input_dir}/'\n        label_folder = f'{input_dir}_labels/'\n\n        label_paths = list_files(label_folder)\n        print(label_paths[0:5])\n\n        for lbl_path in label_paths:\n            image_path = os.path.join(image_folder, os.path.basename(lbl_path) + 'f')\n            if not os.path.exists(image_path):\n                print(f\"{image_path} doesn't exist\")\n                continue\n            image = load_image(os.path.join(image_folder, os.path.basename(lbl_path)+'f'))\n            label = load_image(lbl_path)\n\n            crops, labels = create_crops(image, label, CROP_SIZE)\n\n            for idx, (crop_img, crop_label) in enumerate(zip(crops, labels)):\n                crop_img = crop_img\n                crop_label = binarize(crop_label)\n\n                cv2.imwrite(os.path.join(output_image_dir, f\"crop_{count}.png\"), crop_img)\n                cv2.imwrite(os.path.join(output_label_dir, f\"crop_{count}.png\"), crop_label)\n                count = count+1\n                \nfolders = [\n        '/kaggle/input/massachusetts-roads-dataset/tiff/train',\n        '/kaggle/input/massachusetts-roads-dataset/tiff/test',\n        '/kaggle/input/massachusetts-roads-dataset/tiff/val'\n]\n    \ncrop_and_map_labels(output_train_image_dir, output_train_label_dir, folders[0])\ncrop_and_map_labels(output_test_image_dir, output_test_label_dir, folders[1])\ncrop_and_map_labels(output_val_image_dir, output_val_label_dir, folders[2])","metadata":{"_uuid":"6e3dcdc3-9c55-4444-a291-a89e093d95f9","_cell_guid":"dd8233e2-d0fa-411b-9636-c095ee3b49a3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(os.listdir(output_train_image_dir)))\nprint(len(os.listdir(output_test_image_dir)))      \nprint(len(os.listdir(output_val_image_dir)))      ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Displaying crops for the sake of vanity","metadata":{}},{"cell_type":"code","source":"\ndef display_example_images(output_image_dir, output_label_dir):\n    \"\"\"\n    Display an example image from the output_image_dir and output_label_dir.\n    \"\"\"\n    example_image_path = os.path.join(output_image_dir, \"crop_1.png\")\n    example_label_path = os.path.join(output_label_dir, \"crop_1.png\")\n\n    if os.path.exists(example_image_path) and os.path.exists(example_label_path):\n        image = cv2.imread(example_image_path)\n        label = cv2.imread(example_label_path, cv2.IMREAD_GRAYSCALE)\n\n        # Convert BGR (OpenCV format) to RGB for matplotlib\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        plt.figure(figsize=(10, 5))\n\n        plt.subplot(1, 2, 1)\n        plt.title(\"Example Image\")\n        plt.imshow(image)\n        plt.axis('off')\n\n        plt.subplot(1, 2, 2)\n        plt.title(\"Example Label\")\n        plt.imshow(label, cmap='gray')\n        plt.axis('off')\n\n        plt.show()\n    else:\n        print(\"Example images not found. Ensure at least one crop has been created.\")\n        \ndisplay_example_images(output_test_image_dir, output_test_label_dir)\ndisplay_example_images(output_train_image_dir, output_train_label_dir)\ndisplay_example_images(output_val_image_dir, output_val_label_dir)","metadata":{"_uuid":"2319c8cc-da26-4dd8-aeac-9cc50a7d1299","_cell_guid":"b098a075-9c4d-4a5b-9c3e-c7ae8aa0a623","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing Dataset Generators","metadata":{}},{"cell_type":"code","source":"# Method to create generator\ndef create_generator(zipped):\n    for (img, mask) in zipped:\n        yield (img, mask)\n\n# gen = create_generator(train_generator)\n# first_item,first_item_2 = next(gen)\n# print(first_item,first_item_2)","metadata":{"_uuid":"10fe17d2-1a04-49f6-ab65-2e514948e107","_cell_guid":"47efcf80-3b01-401d-8944-c3dcae018539","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nseed=24\nbatch_size= 16\n\n# Load images for dataset generators from respective dataset libraries. The images and masks are returned as numpy arrays\n\n# Images can be further resized by adding target_size=(150, 150) with any size for your network to flow_from_directory parameters\n# Our images are already cropped to 256x256 so traget_size parameter can be ignored\ndef image_and_mask_generator(image_dir, label_dir):\n    img_data_gen_args = dict(rescale = 1/255.)\n    mask_data_gen_args = dict()\n\n    image_data_generator = ImageDataGenerator(**img_data_gen_args)\n    image_generator = image_data_generator.flow_from_directory(image_dir, \n                                                               seed=seed, \n                                                               batch_size=batch_size,\n                                                               classes = [\".\"],\n                                                               class_mode=None #Very important to set this otherwise it returns multiple numpy arrays thinking class mode is binary.\n                                                               )  \n\n    mask_data_generator = ImageDataGenerator(**mask_data_gen_args)\n    mask_generator = mask_data_generator.flow_from_directory(label_dir, \n                                                             classes = [\".\"],\n                                                             seed=seed, \n                                                             batch_size=batch_size,\n                                                             color_mode = 'grayscale', #Read masks in grayscale\n                                                             class_mode=None\n                                                             )\n    # print processed image paths for vanity\n    print(image_generator.filenames[0:5])\n    print(mask_generator.filenames[0:5])\n    \n    generator = zip(image_generator, mask_generator)\n    return generator\n\ntrain_generator = create_generator(image_and_mask_generator(output_train_image_dir,output_train_label_dir))\ntest_generator = create_generator(image_and_mask_generator(output_test_image_dir,output_test_label_dir))\nval_generator = create_generator(image_and_mask_generator(output_val_image_dir,output_val_label_dir))","metadata":{"_uuid":"fc3e01b2-3c15-4cc4-b877-a1305f7584c6","_cell_guid":"43fa6b8d-1baa-4b23-a7d4-b933a31435e9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining U-NET Model\n### U-NET Model with 4 convolution layers","metadata":{}},{"cell_type":"code","source":"def unet(input_size=(256, 256, 3)):\n    inputs = Input(input_size)\n    \n    # Encoder\n    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n    c1 = BatchNormalization()(c1)\n    c1 = Dropout(0.1)(c1)\n    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n    c1 = BatchNormalization()(c1)\n    p1 = MaxPooling2D((2, 2))(c1)\n    \n    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n    c2 = BatchNormalization()(c2)\n    c2 = Dropout(0.1)(c2)\n    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n    c2 = BatchNormalization()(c2)\n    p2 = MaxPooling2D((2, 2))(c2)\n    \n    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n    c3 = BatchNormalization()(c3)\n    c3 = Dropout(0.2)(c3)\n    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n    c3 = BatchNormalization()(c3)\n    p3 = MaxPooling2D((2, 2))(c3)\n    \n    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n    c4 = BatchNormalization()(c4)\n    c4 = Dropout(0.2)(c4)\n    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n    c4 = BatchNormalization()(c4)\n    p4 = MaxPooling2D((2, 2))(c4)\n    \n    # Bottleneck\n    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n    c5 = BatchNormalization()(c5)\n    c5 = Dropout(0.3)(c5)\n    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n    c5 = BatchNormalization()(c5)\n    \n    # Decoder\n    u6 = UpSampling2D((2, 2))(c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n    c6 = BatchNormalization()(c6)\n    c6 = Dropout(0.2)(c6)\n    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n    c6 = BatchNormalization()(c6)\n    \n    u7 = UpSampling2D((2, 2))(c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n    c7 = BatchNormalization()(c7)\n    c7 = Dropout(0.2)(c7)\n    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n    c7 = BatchNormalization()(c7)\n    \n    u8 = UpSampling2D((2, 2))(c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n    c8 = BatchNormalization()(c8)\n    c8 = Dropout(0.1)(c8)\n    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n    c8 = BatchNormalization()(c8)\n    \n    u9 = UpSampling2D((2, 2))(c8)\n    u9 = concatenate([u9, c1])\n    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n    c9 = BatchNormalization()(c9)\n    c9 = Dropout(0.1)(c9)\n    \n    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n    \n    model = Model(inputs, outputs, name=\"U-NET\")\n    return model","metadata":{"_uuid":"f6831cf8-21c7-420d-bbda-1aacfa7a4331","_cell_guid":"df69c08a-ca29-4361-882c-4c9dcbe474d9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_HEIGHT = IMAGE_WIDTH = CROP_SIZE #256 default","metadata":{"_uuid":"86df4621-0057-4e15-8787-ffcd17b68b10","_cell_guid":"562998b6-33a1-4990-aa15-2091ddfb04db","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define U-NET model's input size with 3 channels\nu_net_model = unet(input_size=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n\n# Plot model summary\nu_net_model.summary()\n\n# Plot model graphically\n# tensorflow.keras.utils.plot_model(u_net_model, show_shapes=True)","metadata":{"_uuid":"75a30c3b-4b02-4458-bcb0-fad38eaad6fa","_cell_guid":"560a404c-5140-43fa-8b34-1b0286ba7feb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Method to calculate Intersection over Union Accuracy Coefficient\ndef iou_coef(y_true, y_pred, smooth=1e-6):\n    intersection = tensorflow.reduce_sum(y_true * y_pred)\n    union = tensorflow.reduce_sum(y_true) + tensorflow.reduce_sum(y_pred) - intersection\n    \n    return (intersection + smooth) / (union + smooth)\n\n# Method to calculate Dice Accuracy Coefficient\ndef dice_coef(y_true, y_pred, smooth=1e-6):\n    intersection = tensorflow.reduce_sum(y_true * y_pred)\n    total = tensorflow.reduce_sum(y_true) + tensorflow.reduce_sum(y_pred)\n    \n    return (2. * intersection + smooth) / (total + smooth)\n\n# Method to calculate Dice Loss\ndef soft_dice_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)","metadata":{"_uuid":"29bc3abb-cb70-47a0-aece-e8aae71222a4","_cell_guid":"f4b3dc3f-2cf9-457f-b0f9-e18eb09b3c90","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define training parameters\nEPOCHS = 5 # 5 epochs default 1 epochs for testing\nLEARNING_RATE = 0.0001\nBATCH_SIZE = 16","metadata":{"_uuid":"440beabb-d18c-41ee-8d7b-d0be23e26101","_cell_guid":"ab61e4a2-4ff0-49ec-be46-1f663c9f68e2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining model path, checkpointer, early stopper and learning rate reducer\nmodel_path = \"/kaggle/working/spectrewolf8/local/aerial-image-road-segmentation-xp/aerial-image-road-segmentation-xp.keras\"\n\ncheckpointer = ModelCheckpoint(model_path, monitor=\"val_loss\", mode=\"min\", save_best_only = True, verbose=1)\nearlystopper = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 1, restore_best_weights = True)\nlr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, epsilon=1e-4)","metadata":{"_uuid":"7f01b0f1-d425-4010-b6cb-2191bef254f8","_cell_guid":"707e9c86-995f-4a3f-acd2-c378aa1b2325","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining optimizer and compiling the model\nopt = keras.optimizers.Adam(LEARNING_RATE)\nu_net_model.compile(optimizer=opt, loss=soft_dice_loss, metrics=[iou_coef])","metadata":{"_uuid":"30e0b6e6-0bbd-467b-bacd-ef8d11070661","_cell_guid":"241c3ea4-91a9-4745-bb69-9a10a8cc3ad1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for available GPUs\ngpus = tensorflow.config.list_physical_devices('GPU')\n\nif gpus:\n    print(f\"GPUs available: {len(gpus)}\")\n    \n#     gpu_devices = tensorflow.config.experimental.list_physical_devices('GPU')\n#     for device in gpu_devices:\n#         tensorflow.config.experimental.set_memory_growth(device, True)\nelse:\n    print(\"No GPUs found.\")","metadata":{"_uuid":"e8c62792-a101-49dc-a62a-a6969d83c79d","_cell_guid":"64d8649e-f9f7-4282-bdab-435658f3a2c3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tarining the model with fit(8)\nhistory = u_net_model.fit(\n    train_generator, \n    epochs=EPOCHS,\n    steps_per_epoch = len(os.listdir(output_train_image_dir)) // BATCH_SIZE,\n    batch_size = BATCH_SIZE,\n    validation_data=val_generator,\n    validation_steps=len(os.listdir(output_val_image_dir)) // BATCH_SIZE,\n    callbacks = [checkpointer, earlystopper, lr_reducer],\n)","metadata":{"_uuid":"b11ea45b-2465-4140-a9cb-06f1f7a679fe","_cell_guid":"fa70c0c9-a8ed-4bbe-9d2e-be400bf62347","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(history.__dict__)\n\n# Ensure that you have more than one epoch's worth of data\nif len(history.history['iou_coef']) > 1:\n    # Plot training & validation accuracy values\n    plt.plot(history.history['iou_coef'], label='Train IOU')\n    plt.plot(history.history['val_iou_coef'], label='Validation IOU')\n    plt.title('Model IOU Coefficient')\n    plt.ylabel('IOU')\n    plt.xlabel('Epoch')\n    plt.legend(loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(loc='upper left')\n    plt.show()\nelse:\n    print(\"Not enough epochs to plot a meaningful graph.\")\n","metadata":{"_uuid":"0fc6de73-64ef-4075-b328-acd9f3789123","_cell_guid":"ab120a7a-b3e4-4b01-a39a-99dbc2e15bcd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"u_net_model.save(model_path, save_format='keras')","metadata":{"_uuid":"27e7924e-e8b4-49d6-91f4-c27e8332a479","_cell_guid":"7b7a3961-9841-4982-91d7-e250ba69d971","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Upload the model to Hugging Face Hub","metadata":{}},{"cell_type":"code","source":"#load tokens\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"_uuid":"bc59da1f-ceac-49d8-b8c0-97e8454514ae","_cell_guid":"3d9b0905-dade-482f-ac32-d329f8e124f5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi, HfFolder, Repository, notebook_login\n\n# Define repository name and local path\nrepo_name = \"spectrewolf8/aerial-image-road-segmentation-xp\"\nrepo_path = \"/kaggle/working/spectrewolf8/hub/aerial-image-road-segmentation-xp\"\nmodel_path = \"/kaggle/working/spectrewolf8/local/aerial-image-road-segmentation-xp/aerial-image-road-segmentation-xp.keras\"\n\n#logging into Hugging Face\n!huggingface-cli login --token $hf_token\n\n# Create a repository\napi = HfApi()\n\napi.create_repo(repo_id=repo_name, exist_ok=True)\n\n# Deleting repo files if they exist already\n!rm -rf $repo_path\n\n# Initialize the local repository\nrepo = Repository(local_dir=repo_path, clone_from=repo_name)\n\n# Copy the model file to the repository directory\nimport shutil\nshutil.copy(model_path, repo_path)  # or .h5 file\n\n# Push to the Hub\nrepo.push_to_hub()","metadata":{"_uuid":"2bd37721-ba93-4dc4-b2f8-1f89eb2a2b21","_cell_guid":"fd9d73c8-8fd3-448b-a131-9a78b4d85968","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download the model from Hugging Face Hub","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nimport tensorflow as tf\n\n# Define your model repository and file name\nrepo_name = \"spectrewolf8/aerial-image-road-segmentation-xp\"\nfilename = \"aerial-image-road-segmentation-xp.keras\"  # Update this to the actual model file name\n\n# Define the directory where you want to save the downloaded model\ndownload_dir = \"/kaggle/working/spectrewolf8/cache/\"\n\n# Download the model file from Hugging Face Hub to the specified directory\nlocal_model_path = hf_hub_download(repo_id=repo_name, filename=filename, cache_dir=download_dir)\n\n# !rm - rf /kaggle/working/spectrewolf8/cache/models--spectrewolf8--aerial-image-road-segmentation-xp\n\n\n# Define the custom objects used in your model\n# These should match the custom objects used during model training\ncustom_objects = {\n    'soft_dice_loss': soft_dice_loss,\n    'dice_coef': dice_coef,\n    \"iou_coef\": iou_coef\n}\n\n# Load the model with custom objects\nmodel = tf.keras.models.load_model(local_model_path, custom_objects=custom_objects)\n\n\n# Check the model summary\nmodel.summary()","metadata":{"_uuid":"af4e6ca0-64d0-4b08-a3ae-f8f6cd624af1","_cell_guid":"b06c3012-91ff-428f-918b-4b9460621dd7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate and test Model","metadata":{}},{"cell_type":"markdown","source":"## Evaluate Model","metadata":{}},{"cell_type":"code","source":"# Import necessary classes\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras.models import Sequential","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Method to calculate Intersection over Union Accuracy Coefficient\ndef iou_coef(y_true, y_pred, smooth=1e-6):\n    intersection = tensorflow.reduce_sum(y_true * y_pred)\n    union = tensorflow.reduce_sum(y_true) + tensorflow.reduce_sum(y_pred) - intersection\n    \n    return (intersection + smooth) / (union + smooth)\n\n# Method to calculate Dice Accuracy Coefficient\ndef dice_coef(y_true, y_pred, smooth=1e-6):\n    intersection = tensorflow.reduce_sum(y_true * y_pred)\n    total = tensorflow.reduce_sum(y_true) + tensorflow.reduce_sum(y_pred)\n    \n    return (2. * intersection + smooth) / (total + smooth)\n\n# Method to calculate Dice Loss\ndef soft_dice_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)","metadata":{"_uuid":"2df5d5e9-db0d-493e-8579-877a96b1edce","_cell_guid":"e72f7bd5-9068-4dae-ab70-e960162d390b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Method to create generator\ndef create_generator(zipped):\n    for (img, mask) in zipped:\n        yield (img, mask)","metadata":{"_uuid":"10fe17d2-1a04-49f6-ab65-2e514948e107","_cell_guid":"47efcf80-3b01-401d-8944-c3dcae018539","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = \"/kaggle/working/spectrewolf8/local/aerial-image-road-segmentation-xp/aerial-image-road-segmentation-xp.keras\"\nu_net_model = load_model(model_path, custom_objects={'soft_dice_loss': soft_dice_loss, 'dice_coef': dice_coef, \"iou_coef\": iou_coef})\n\n# Evaluate the model\nloss, iou = u_net_model.evaluate(val_generator, steps=100) # Any number of steps can be chosed. \n\n# print(u_net_model.evaluate())\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test IOU: {iou}\")","metadata":{"_uuid":"a7796c36-dfe9-465a-9f80-f730f905cfc0","_cell_guid":"082df7b6-fd31-4bab-a4ab-9b918a31b0b7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test model ","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming create_generator is defined and provides images for prediction\nimages, ground_truth_masks = next(test_generator)\n\n# Make predictions\npredictions = u_net_model.predict(images)\n\n# Apply threshold to predictions\nthresh_val = 0.8\nprediction_threshold = (predictions > thresh_val).astype(np.uint8)\n\n# Visualize results\nnum_samples = min(10, len(images))  # Use at most 10 samples or the total number of images available\nf = plt.figure(figsize=(15, 25))\nfor i in range(num_samples):\n    ix = random.randint(0, len(images) - 1)  # Ensure ix is within range\n\n    f.add_subplot(num_samples, 4, i * 4 + 1)\n    plt.imshow(images[ix])\n    plt.title(\"Image\")\n    plt.axis('off')\n\n    f.add_subplot(num_samples, 4, i * 4 + 2)\n    plt.imshow(np.squeeze(ground_truth_masks[ix]))\n    plt.title(\"Ground Truth\")\n    plt.axis('off')\n\n    f.add_subplot(num_samples, 4, i * 4 + 3)\n    plt.imshow(np.squeeze(predictions[ix]))\n    plt.title(\"Prediction\")\n    plt.axis('off')\n\n    f.add_subplot(num_samples, 4, i * 4 + 4)\n    plt.imshow(np.squeeze(prediction_threshold[ix]))\n    plt.title(f\"Thresholded at {thresh_val}\")\n    plt.axis('off')\n\nplt.show()\n","metadata":{"_uuid":"ce0f65d7-f80f-4ba7-8283-b3510f71c0f2","_cell_guid":"93aacabc-ce2f-4e48-9b22-cb8c31dca6eb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}